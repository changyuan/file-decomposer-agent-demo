#!/usr/bin/env python3
"""
å¤§æ–‡ä»¶åˆ†è§£å’Œç†è§£ Agent
åŸºäº LangChain v1.0 å’Œ deepagents æ¡†æ¶
æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼çš„æ™ºèƒ½åˆ†è§£å’Œå†…å®¹åˆ†æ
"""

import os
import json
import logging
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Union
from pathlib import Path
import hashlib

# LangChain v1.0 æ ¸å¿ƒç»„ä»¶
from langchain.agents import create_agent, AgentState
from langchain.agents.middleware import (
    TodoListMiddleware,
    ContextEditingMiddleware,
    ClearToolUsesEdit,
    PIIMiddleware
)
from langchain.messages import HumanMessage, AIMessage
from langchain.tools import tool, ToolRuntime
from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings

# æ–‡æ¡£åŠ è½½å’Œåˆ†å‰²
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    CSVLoader,
    UnstructuredHTMLLoader,
    JSONLoader
)
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document

# å‘é‡å­˜å‚¨
from langchain_community.vectorstores import FAISS

# å·¥å…·å’Œè¿è¡Œæ—¶
from langgraph.store.memory import InMemoryStore

# æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class FileContext:
    """æ–‡ä»¶å¤„ç†ä¸Šä¸‹æ–‡"""
    file_path: str
    file_type: str
    file_size: int
    chunk_count: int = 0
    processed_at: Optional[str] = None
    content_hash: Optional[str] = None
    analysis_results: Dict[str, Any] = field(default_factory=dict)

class FileDecomposerAgent:
    """å¤§æ–‡ä»¶åˆ†è§£å’Œç†è§£ Agent"""

    def __init__(self, model_name: str = "claude-sonnet-4-5-20250929"):
        """
        åˆå§‹åŒ–æ–‡ä»¶åˆ†è§£ Agent

        Args:
            model_name: ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.vector_store = None
        self.embeddings = None
        self.text_splitter = None
        self.store = InMemoryStore()
        self.agent = None
        self.file_contexts: Dict[str, FileContext] = {}

        # åˆå§‹åŒ–ç»„ä»¶
        self._initialize_components()

    def _initialize_components(self):
        """åˆå§‹åŒ– LangChain ç»„ä»¶"""
        try:
            # åˆå§‹åŒ–è¯­è¨€æ¨¡å‹
            self.model = init_chat_model(self.model_name)

            # åˆå§‹åŒ–æ–‡æœ¬åµŒå…¥æ¨¡å‹
            self.embeddings = init_embeddings("openai:text-embedding-3-small")

            # åˆå§‹åŒ–æ–‡æœ¬åˆ†å‰²å™¨
            self.text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                add_start_index=True,
                length_function=len,
            )

            # åˆ›å»º Agent
            self._create_agent()

            logger.info("æ–‡ä»¶åˆ†è§£ Agent åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def _create_agent(self):
        """åˆ›å»ºå¸¦æœ‰ä¸­é—´ä»¶çš„ Agent"""

        # å®šä¹‰ Agent å·¥å…·
        tools = [
            self._load_file_tool,
            self._split_document_tool,
            self._analyze_content_tool,
            self._search_similar_content_tool,
            self._generate_summary_tool,
            self._extract_key_info_tool,
        ]

        # é…ç½®ä¸­é—´ä»¶
        middleware = [
            TodoListMiddleware(),
            ContextEditingMiddleware(
                edits=[
                    ClearToolUsesEdit(
                        trigger=2000,
                        keep=3,
                        clear_tool_inputs=False,
                        exclude_tools=["load_file"]
                    )
                ]
            ),
            PIIMiddleware(
                "email",
                strategy="redact",
                apply_to_input=True
            ),
        ]

        # ç³»ç»Ÿæç¤ºè¯
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ä»¶åˆ†æåŠ©æ‰‹ï¼Œä¸“é—¨è´Ÿè´£å¤§æ–‡ä»¶çš„åˆ†è§£å’Œç†è§£ã€‚

        æ ¸å¿ƒèƒ½åŠ›ï¼š
        1. åŠ è½½å’Œè§£æå¤šç§æ ¼å¼çš„æ–‡ä»¶ï¼ˆPDF, TXT, CSV, HTML, JSONç­‰ï¼‰
        2. æ™ºèƒ½åˆ†å‰²å¤§æ–‡ä»¶ä¸ºå¯ç®¡ç†çš„å—
        3. åˆ†ææ–‡ä»¶å†…å®¹ç»“æ„å’Œä¸»é¢˜
        4. æä¾›å†…å®¹æ‘˜è¦å’Œå…³é”®ä¿¡æ¯æå–
        5. å›ç­”å…³äºæ–‡ä»¶å†…å®¹çš„å…·ä½“é—®é¢˜

        å·¥ä½œæµç¨‹ï¼š
        - é¦–å…ˆåŠ è½½æ–‡ä»¶å¹¶æ£€æµ‹æ–‡ä»¶ç±»å‹
        - ä½¿ç”¨é€‚å½“çš„åŠ è½½å™¨è§£ææ–‡ä»¶
        - å°†å†…å®¹åˆ†å‰²æˆè¾ƒå°çš„å—
        - ä¸ºæ¯ä¸ªå—ç”Ÿæˆå‘é‡åµŒå…¥
        - å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ä¸­
        - æä¾›åˆ†æå’ŒæŸ¥è¯¢åŠŸèƒ½

        å›ç­”è¦æ±‚ï¼š
        - å‡†ç¡®åˆ†ææ–‡ä»¶å†…å®¹
        - æä¾›ç»“æ„åŒ–çš„åˆ†æç»“æœ
        - åœ¨ä¸ç¡®å®šæ—¶æ˜ç¡®è¯´æ˜
        - ä¿æŠ¤æ•æ„Ÿä¿¡æ¯
        """

        # åˆ›å»º Agent
        self.agent = create_agent(
            model=self.model,
            tools=tools,
            middleware=middleware,
            system_prompt=system_prompt,
            store=self.store
        )

    @tool
    def _load_file_tool(self, file_path: str) -> str:
        """åŠ è½½æ–‡ä»¶å·¥å…· - æ”¯æŒå¤šç§æ ¼å¼"""
        try:
            file_path = Path(file_path)

            if not file_path.exists():
                return f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} ä¸å­˜åœ¨"

            # æ£€æµ‹æ–‡ä»¶ç±»å‹
            file_extension = file_path.suffix.lower()

            # åˆ›å»ºæ–‡æ¡£åŠ è½½å™¨
            if file_extension == '.pdf':
                loader = PyPDFLoader(str(file_path))
                docs = loader.load()
                content_type = "PDFæ–‡æ¡£"

            elif file_extension == '.txt':
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()
                content_type = "æ–‡æœ¬æ–‡ä»¶"

            elif file_extension == '.csv':
                loader = CSVLoader(str(file_path))
                docs = loader.load()
                content_type = "CSVæ•°æ®æ–‡ä»¶"

            elif file_extension == '.html':
                loader = UnstructuredHTMLLoader(str(file_path))
                docs = loader.load()
                content_type = "HTMLç½‘é¡µ"

            elif file_extension == '.json':
                loader = JSONLoader(str(file_path), jq_schema='.')
                docs = loader.load()
                content_type = "JSONæ•°æ®"

            elif file_extension in ['.md', '.markdown']:
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()
                content_type = "Markdownæ–‡æ¡£"

            else:
                # é»˜è®¤ä½¿ç”¨æ–‡æœ¬åŠ è½½å™¨
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()
                content_type = f"{file_extension[1:].upper()}æ–‡ä»¶"

            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            with open(file_path, 'rb') as f:
                file_hash = hashlib.md5(f.read()).hexdigest()

            # åˆ›å»ºæ–‡ä»¶ä¸Šä¸‹æ–‡
            file_context = FileContext(
                file_path=str(file_path),
                file_type=content_type,
                file_size=file_path.stat().st_size,
                content_hash=file_hash
            )

            self.file_contexts[str(file_path)] = file_context

            logger.info(f"æˆåŠŸåŠ è½½ {content_type}: {file_path}")

            return f"""
            æ–‡ä»¶åŠ è½½æˆåŠŸï¼
            æ–‡ä»¶è·¯å¾„: {file_path}
            æ–‡ä»¶ç±»å‹: {content_type}
            æ–‡ä»¶å¤§å°: {file_path.stat().st_size:,} å­—èŠ‚
            æ–‡æ¡£æ•°é‡: {len(docs)}
            æ–‡ä»¶å“ˆå¸Œ: {file_hash[:8]}...

            ç¤ºä¾‹å†…å®¹é¢„è§ˆ:
            {docs[0].page_content[:300] if docs else "æ— å†…å®¹"}
            """

        except Exception as e:
            logger.error(f"åŠ è½½æ–‡ä»¶å¤±è´¥: {e}")
            return f"åŠ è½½æ–‡ä»¶å¤±è´¥: {str(e)}"

    @tool
    def _split_document_tool(self, file_path: str, chunk_size: int = 1000, chunk_overlap: int = 200) -> str:
        """åˆ†å‰²æ–‡æ¡£å·¥å…·"""
        try:
            file_path = Path(file_path)

            if str(file_path) not in self.file_contexts:
                return f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} æœªåŠ è½½ï¼Œè¯·å…ˆä½¿ç”¨ load_file å·¥å…·"

            # é‡æ–°åŠ è½½æ–‡ä»¶
            loader = TextLoader(str(file_path), encoding='utf-8')
            docs = loader.load()

            # ä½¿ç”¨æ–‡æœ¬åˆ†å‰²å™¨
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                add_start_index=True
            )

            # åˆ†å‰²æ–‡æ¡£
            all_splits = text_splitter.split_documents(docs)

            # æ›´æ–°æ–‡ä»¶ä¸Šä¸‹æ–‡
            file_context = self.file_contexts[str(file_path)]
            file_context.chunk_count = len(all_splits)

            logger.info(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼š{len(all_splits)} ä¸ªå—")

            return f"""
            æ–‡æ¡£åˆ†å‰²å®Œæˆï¼
            åŸå§‹æ–‡æ¡£æ•°: {len(docs)}
            åˆ†å‰²å—æ•°é‡: {len(all_splits)}
            å—å¤§å°: {chunk_size} å­—ç¬¦
            é‡å å­—ç¬¦: {chunk_overlap}

            å‰3ä¸ªå—çš„é¢„è§ˆ:
            """

            # æ·»åŠ å—é¢„è§ˆ
            for i, split in enumerate(all_splits[:3]):
                return += f"""
                å— {i+1} (èµ·å§‹ä½ç½®: {split.metadata.get('start_index', 'N/A')}):
                {split.page_content[:200]}...
                """

        except Exception as e:
            logger.error(f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {e}")
            return f"æ–‡æ¡£åˆ†å‰²å¤±è´¥: {str(e)}"

    @tool
    def _analyze_content_tool(self, file_path: str) -> str:
        """åˆ†ææ–‡ä»¶å†…å®¹å·¥å…·"""
        try:
            if str(file_path) not in self.file_contexts:
                return f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} æœªåŠ è½½"

            file_context = self.file_contexts[str(file_path)]
            file_path_obj = Path(file_path)

            # åŠ è½½æ–‡ä»¶å†…å®¹
            if file_path_obj.suffix == '.pdf':
                loader = PyPDFLoader(str(file_path_obj))
                docs = loader.load()
            else:
                loader = TextLoader(str(file_path_obj), encoding='utf-8')
                docs = loader.load()

            # åŸºç¡€ç»Ÿè®¡
            total_chars = sum(len(doc.page_content) for doc in docs)
            total_words = sum(len(doc.page_content.split()) for doc in docs)
            total_lines = sum(doc.page_content.count('\n') + 1 for doc in docs)

            # å†…å®¹åˆ†æ
            analysis = {
                "æ–‡ä»¶ä¿¡æ¯": {
                    "è·¯å¾„": file_context.file_path,
                    "ç±»å‹": file_context.file_type,
                    "å¤§å°": f"{file_context.file_size:,} å­—èŠ‚",
                    "å“ˆå¸Œ": file_context.content_hash[:8] + "..." if file_context.content_hash else "N/A"
                },
                "å†…å®¹ç»Ÿè®¡": {
                    "æ–‡æ¡£æ•°": len(docs),
                    "å­—ç¬¦æ•°": total_chars,
                    "å•è¯æ•°": total_words,
                    "è¡Œæ•°": total_lines,
                    "å—æ•°": file_context.chunk_count
                }
            }

            # æ›´æ–°ä¸Šä¸‹æ–‡
            file_context.analysis_results = analysis

            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            report = "## æ–‡ä»¶å†…å®¹åˆ†ææŠ¥å‘Š\n\n"
            for section, data in analysis.items():
                report += f"### {section}\n"
                for key, value in data.items():
                    report += f"- **{key}**: {value}\n"
                report += "\n"

            logger.info(f"æ–‡ä»¶åˆ†æå®Œæˆ: {file_path}")

            return report

        except Exception as e:
            logger.error(f"å†…å®¹åˆ†æå¤±è´¥: {e}")
            return f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}"

    @tool
    def _search_similar_content_tool(self, query: str, file_path: Optional[str] = None, k: int = 3) -> str:
        """æœç´¢ç›¸ä¼¼å†…å®¹å·¥å…·"""
        try:
            # å¦‚æœæŒ‡å®šäº†æ–‡ä»¶ï¼Œåªåœ¨è¯¥æ–‡ä»¶ä¸­æœç´¢
            if file_path and str(file_path) in self.file_contexts:
                # TODO: å®ç°æ–‡ä»¶ç‰¹å®šçš„æœç´¢
                return "æ–‡ä»¶ç‰¹å®šæœç´¢åŠŸèƒ½å¾…å®ç°"

            # å¦‚æœæ²¡æœ‰å‘é‡å­˜å‚¨ï¼Œæç¤ºå…ˆå»ºç«‹ç´¢å¼•
            if self.vector_store is None:
                return "é”™è¯¯ï¼šæœªå»ºç«‹å‘é‡ç´¢å¼•ã€‚è¯·å…ˆåŠ è½½æ–‡ä»¶å¹¶åˆ†å‰²æ–‡æ¡£ã€‚"

            # æ‰§è¡Œç›¸ä¼¼æ€§æœç´¢
            results = self.vector_store.similarity_search(query, k=k)

            if not results:
                return f"æœªæ‰¾åˆ°ä¸æŸ¥è¯¢ '{query}' ç›¸ä¼¼çš„å†…å®¹"

            # æ ¼å¼åŒ–ç»“æœ
            response = f"## æœç´¢ç»“æœï¼š'{query}'\n\næ‰¾åˆ° {len(results)} ä¸ªç›¸å…³å†…å®¹ï¼š\n\n"

            for i, doc in enumerate(results, 1):
                response += f"""
                ### ç»“æœ {i}
                **å†…å®¹**: {doc.page_content[:300]}...
                **å…ƒæ•°æ®**: {doc.metadata}
                **ç›¸ä¼¼åº¦**: {getattr(doc, 'similarity_score', 'N/A')}
                ---
                """

            return response

        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return f"æœç´¢å¤±è´¥: {str(e)}"

    @tool
    def _generate_summary_tool(self, file_path: str) -> str:
        """ç”Ÿæˆæ–‡ä»¶æ‘˜è¦å·¥å…·"""
        try:
            if str(file_path) not in self.file_contexts:
                return f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} æœªåŠ è½½"

            # ä½¿ç”¨ Agent ç”Ÿæˆæ‘˜è¦
            query = f"è¯·ä¸ºæ–‡ä»¶ {file_path} ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„æ‘˜è¦ï¼ŒåŒ…æ‹¬ä¸»è¦å†…å®¹å’Œå…³é”®ä¿¡æ¯ã€‚"

            response = self.agent.invoke({
                'messages': [HumanMessage(content=query)]
            })

            summary = response['messages'][-1].content

            return f"## æ–‡ä»¶æ‘˜è¦: {file_path}\n\n{summary}"

        except Exception as e:
            logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"

    @tool
    def _extract_key_info_tool(self, file_path: str, info_type: str = "main_points") -> str:
        """æå–å…³é”®ä¿¡æ¯å·¥å…·"""
        try:
            if str(file_path) not in self.file_contexts:
                return f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} æœªåŠ è½½"

            # å®šä¹‰ä¸åŒç±»å‹çš„ä¿¡æ¯æå–
            extract_prompts = {
                "main_points": "è¯·æå–æ–‡ä»¶çš„ä¸»è¦è§‚ç‚¹å’Œè¦ç‚¹",
                "names": "è¯·æå–æ–‡ä»¶ä¸­çš„æ‰€æœ‰é‡è¦åç§°ã€äººåã€åœ°åç­‰å®ä½“",
                "dates": "è¯·æå–æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ—¥æœŸã€æ—¶é—´ä¿¡æ¯",
                "numbers": "è¯·æå–æ–‡ä»¶ä¸­çš„æ‰€æœ‰æ•°å­—ã€ç»Ÿè®¡æ•°æ®ç­‰",
                "questions": "è¯·æå–æ–‡ä»¶ä¸­çš„æ‰€æœ‰é—®é¢˜æˆ–ç–‘é—®",
                "actions": "è¯·æå–æ–‡ä»¶ä¸­çš„æ‰€æœ‰è¡ŒåŠ¨é¡¹ç›®æˆ–å»ºè®®"
            }

            if info_type not in extract_prompts:
                return f"ä¸æ”¯æŒçš„ä¿¡æ¯ç±»å‹: {info_type}ã€‚æ”¯æŒçš„ç±»å‹: {list(extract_prompts.keys())}"

            # ä½¿ç”¨ Agent æå–ä¿¡æ¯
            query = f"{extract_prompts[info_type]}ï¼Œæ–‡ä»¶è·¯å¾„: {file_path}"

            response = self.agent.invoke({
                'messages': [HumanMessage(content=query)]
            })

            extracted_info = response['messages'][-1].content

            return f"## {info_type.replace('_', ' ').title()}: {file_path}\n\n{extracted_info}"

        except Exception as e:
            logger.error(f"ä¿¡æ¯æå–å¤±è´¥: {e}")
            return f"ä¿¡æ¯æå–å¤±è´¥: {str(e)}"

    def build_vector_index(self, file_path: str) -> str:
        """å»ºç«‹å‘é‡ç´¢å¼•"""
        try:
            if str(file_path) not in self.file_contexts:
                return f"é”™è¯¯ï¼šæ–‡ä»¶ {file_path} æœªåŠ è½½"

            file_path_obj = Path(file_path)

            # åŠ è½½å’Œåˆ†å‰²æ–‡æ¡£
            loader = TextLoader(str(file_path_obj), encoding='utf-8')
            docs = loader.load()

            all_splits = self.text_splitter.split_documents(docs)

            # åˆ›å»ºå‘é‡å­˜å‚¨
            self.vector_store = FAISS.from_documents(all_splits, self.embeddings)

            logger.info(f"å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆ: {len(all_splits)} ä¸ªå‘é‡")

            return f"å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆï¼\næ–‡æ¡£å—æ•°: {len(all_splits)}\nå‘é‡ç»´åº¦: {self.embeddings.dimensionality}"

        except Exception as e:
            logger.error(f"å‘é‡ç´¢å¼•å»ºç«‹å¤±è´¥: {e}")
            return f"å‘é‡ç´¢å¼•å»ºç«‹å¤±è´¥: {str(e)}"

    def chat_with_file(self, file_path: str, question: str) -> str:
        """ä¸æ–‡ä»¶å¯¹è¯"""
        try:
            if not self.vector_store:
                # å¦‚æœæ²¡æœ‰å‘é‡ç´¢å¼•ï¼Œå…ˆå»ºç«‹ç´¢å¼•
                self.build_vector_index(file_path)

            # ä½¿ç”¨ Agent å›ç­”é—®é¢˜
            query = f"åŸºäºæ–‡ä»¶ {file_path} çš„å†…å®¹ï¼Œè¯·å›ç­”ä»¥ä¸‹é—®é¢˜: {question}"

            response = self.agent.invoke({
                'messages': [HumanMessage(content=query)]
            })

            answer = response['messages'][-1].content

            return f"## é—®é¢˜: {question}\n\n**ç­”æ¡ˆ**: {answer}"

        except Exception as e:
            logger.error(f"å¯¹è¯å¤±è´¥: {e}")
            return f"å¯¹è¯å¤±è´¥: {str(e)}"

    def get_file_status(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰æ–‡ä»¶çš„å¤„ç†çŠ¶æ€"""
        status = {}
        for file_path, context in self.file_contexts.items():
            status[file_path] = {
                "file_type": context.file_type,
                "file_size": context.file_size,
                "chunk_count": context.chunk_count,
                "analysis_results": context.analysis_results
            }
        return status


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ–‡ä»¶åˆ†è§£ Agent"""
    # åˆ›å»º Agent å®ä¾‹
    agent = FileDecomposerAgent()

    print("ğŸš€ å¤§æ–‡ä»¶åˆ†è§£å’Œç†è§£ Agent å·²å¯åŠ¨")
    print("=" * 50)

    # æ¼”ç¤ºç”¨æ³•
    example_files = [
        "/path/to/your/document.pdf",
        "/path/to/your/text.txt",
        "/path/to/your/data.csv"
    ]

    print("\nğŸ“– ä½¿ç”¨ç¤ºä¾‹:")
    print("1. åŠ è½½æ–‡ä»¶: agent._load_file_tool('/path/to/your/file.txt')")
    print("2. åˆ†å‰²æ–‡æ¡£: agent._split_document_tool('/path/to/your/file.txt')")
    print("3. åˆ†æå†…å®¹: agent._analyze_content_tool('/path/to/your/file.txt')")
    print("4. å»ºç«‹å‘é‡ç´¢å¼•: agent.build_vector_index('/path/to/your/file.txt')")
    print("5. ä¸æ–‡ä»¶å¯¹è¯: agent.chat_with_file('/path/to/your/file.txt', 'è¿™ä¸ªæ–‡ä»¶çš„ä¸»è¦å†…å®¹æ˜¯ä»€ä¹ˆï¼Ÿ')")

    print("\nğŸ”§ å¯ç”¨çš„ä¿¡æ¯æå–ç±»å‹:")
    extract_types = ["main_points", "names", "dates", "numbers", "questions", "actions"]
    for i, extract_type in enumerate(extract_types, 1):
        print(f"{i}. {extract_type}")

    print("\nğŸ’¡ æç¤º: è¯·å°†ä¸Šè¿°ç¤ºä¾‹æ–‡ä»¶è·¯å¾„æ›¿æ¢ä¸ºæ‚¨çš„å®é™…æ–‡ä»¶è·¯å¾„")


if __name__ == "__main__":
    main()
#!/usr/bin/env python3
"""
å¤§æ–‡ä»¶åˆ†è§£å’Œç†è§£ Agent ä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ LangChain v1.0 å’Œ deepagents æ¡†æ¶æ¥åˆ†ææ–‡ä»¶
"""

import os
from pathlib import Path
from file_decomposer_agent import FileDecomposerAgent, HumanMessage

def create_sample_files():
    """åˆ›å»ºç¤ºä¾‹æ–‡ä»¶ç”¨äºæµ‹è¯•"""
    sample_dir = Path("sample_files")
    sample_dir.mkdir(exist_ok=True)

    # åˆ›å»ºç¤ºä¾‹æ–‡æœ¬æ–‡ä»¶
    sample_text = """
    äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹

    äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒè¯•å›¾ç†è§£æ™ºèƒ½çš„å®è´¨ï¼Œ
    å¹¶ç”Ÿäº§å‡ºä¸€ç§æ–°çš„èƒ½ä»¥äººç±»æ™ºèƒ½ç›¸ä¼¼çš„æ–¹å¼åšå‡ºååº”çš„æ™ºèƒ½æœºå™¨ã€‚

    ## å†å²å‘å±•

    äººå·¥æ™ºèƒ½çš„å‘å±•å¯ä»¥è¿½æº¯åˆ°20ä¸–çºª40å¹´ä»£ã€‚1943å¹´ï¼ŒMcCullochå’ŒPittsæå‡ºäº†ç¬¬ä¸€ä¸ªäººå·¥ç¥ç»å…ƒæ¨¡å‹ã€‚
    1956å¹´ï¼Œåœ¨è¾¾ç‰¹èŒ…æ–¯ä¼šè®®ä¸Šï¼Œ"äººå·¥æ™ºèƒ½"è¿™ä¸€æœ¯è¯­æ­£å¼è¢«æå‡ºã€‚

    ## å‘å±•é˜¶æ®µ

    ### ç¬¬ä¸€é˜¶æ®µï¼ˆ1950å¹´ä»£-1960å¹´ä»£ï¼‰
    è¿™ä¸€é˜¶æ®µä¸»è¦æ˜¯ç¬¦å·ä¸»ä¹‰AIçš„å‘å±•ï¼Œä¸»è¦é›†ä¸­åœ¨é€»è¾‘æ¨ç†å’ŒçŸ¥è¯†è¡¨ç¤ºæ–¹é¢ã€‚

    ### ç¬¬äºŒé˜¶æ®µï¼ˆ1970å¹´ä»£-1980å¹´ä»£ï¼‰
    ä¸“å®¶ç³»ç»Ÿçš„å…´èµ·ï¼ŒåŸºäºçŸ¥è¯†çš„AIç³»ç»Ÿå¼€å§‹åœ¨ç‰¹å®šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ã€‚

    ### ç¬¬ä¸‰é˜¶æ®µï¼ˆ1990å¹´ä»£-2000å¹´ä»£ï¼‰
    æœºå™¨å­¦ä¹ ç®—æ³•çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯ç»Ÿè®¡å­¦ä¹ æ–¹æ³•çš„åº”ç”¨ã€‚

    ### ç¬¬å››é˜¶æ®µï¼ˆ2010å¹´ä»£è‡³ä»Šï¼‰
    æ·±åº¦å­¦ä¹ çš„å…´èµ·ï¼ŒåŸºäºç¥ç»ç½‘ç»œçš„AIç³»ç»Ÿå–å¾—äº†çªç ´æ€§è¿›å±•ã€‚

    ## å½“å‰åº”ç”¨

    äººå·¥æ™ºèƒ½åœ¨è®¸å¤šé¢†åŸŸéƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼š
    - è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰
    - è®¡ç®—æœºè§†è§‰
    - æ¨èç³»ç»Ÿ
    - è‡ªåŠ¨é©¾é©¶
    - åŒ»ç–—è¯Šæ–­

    ## æœªæ¥å±•æœ›

    éšç€æŠ€æœ¯çš„ä¸æ–­å‘å±•ï¼Œäººå·¥æ™ºèƒ½å°†åœ¨æ›´å¤šé¢†åŸŸå‘æŒ¥é‡è¦ä½œç”¨ï¼Œ
    ä¸ºäººç±»ç¤¾ä¼šå¸¦æ¥æ›´å¤§çš„ä»·å€¼ã€‚
    """

    with open(sample_dir / "ai_history.txt", "w", encoding="utf-8") as f:
        f.write(sample_text)

    # åˆ›å»ºç¤ºä¾‹CSVæ–‡ä»¶
    sample_csv = """å…¬å¸,æˆç«‹å¹´ä»½,åˆ›å§‹äºº,æ€»éƒ¨ä½ç½®,ä¸»è¦äº§å“
Google,1998,æ‹‰é‡ŒÂ·ä½©å¥‡,åŠ åˆ©ç¦å°¼äºš,æœç´¢å¼•æ“
Microsoft,1975,æ¯”å°”Â·ç›–èŒ¨,åç››é¡¿å·,æ“ä½œç³»ç»Ÿ
Apple,1976,å²è’‚å¤«Â·ä¹”å¸ƒæ–¯,åŠ åˆ©ç¦å°¼äºš,æ¶ˆè´¹ç”µå­äº§å“
Amazon,1994,æ°å¤«Â·è´ç´¢æ–¯,åç››é¡¿å·,ç”µå­å•†åŠ¡
Tesla,2003,åŸƒéš†Â·é©¬æ–¯å…‹,åŠ åˆ©ç¦å°¼äºš,ç”µåŠ¨æ±½è½¦
"""

    with open(sample_dir / "tech_companies.csv", "w", encoding="utf-8") as f:
        f.write(sample_csv)

    # åˆ›å»ºç¤ºä¾‹JSONæ–‡ä»¶
    sample_json = {
        "é¡¹ç›®ä¿¡æ¯": {
            "åç§°": "å¤§æ–‡ä»¶åˆ†è§£ç³»ç»Ÿ",
            "ç‰ˆæœ¬": "1.0.0",
            "å¼€å‘è€…": "AIåŠ©æ‰‹",
            "åˆ›å»ºæ—¥æœŸ": "2024-12-19"
        },
        "åŠŸèƒ½åˆ—è¡¨": [
            "æ–‡ä»¶åŠ è½½å’Œè§£æ",
            "æ™ºèƒ½æ–‡æ¡£åˆ†å‰²",
            "å†…å®¹åˆ†æå’Œæ‘˜è¦",
            "å‘é‡ç›¸ä¼¼æ€§æœç´¢",
            "äº¤äº’å¼é—®ç­”"
        ],
        "æŠ€æœ¯æ ˆ": {
            "æ ¸å¿ƒæ¡†æ¶": "LangChain v1.0",
            "å‘é‡æ•°æ®åº“": "FAISS",
            "åµŒå…¥æ¨¡å‹": "OpenAI text-embedding-3-small",
            "è¯­è¨€æ¨¡å‹": "Claude Sonnet"
        },
        "æ€§èƒ½æŒ‡æ ‡": {
            "æ”¯æŒæ–‡ä»¶ç±»å‹": ["PDF", "TXT", "CSV", "HTML", "JSON", "MD"],
            "æœ€å¤§æ–‡ä»¶å¤§å°": "100MB",
            "å¤„ç†é€Ÿåº¦": "1000å­—ç¬¦/ç§’",
            "å‡†ç¡®æ€§": "95%+"
        }
    }

    with open(sample_dir / "project_config.json", "w", encoding="utf-8") as f:
        import json
        json.dump(sample_json, f, ensure_ascii=False, indent=2)

    print(f"ç¤ºä¾‹æ–‡ä»¶å·²åˆ›å»ºåœ¨ {sample_dir} ç›®å½•ä¸­")
    return sample_dir

def demo_file_analysis():
    """æ¼”ç¤ºæ–‡ä»¶åˆ†æåŠŸèƒ½"""
    print("\n" + "="*60)
    print("ğŸš€ å¤§æ–‡ä»¶åˆ†è§£å’Œç†è§£ Agent æ¼”ç¤º")
    print("="*60)

    # åˆ›å»ºç¤ºä¾‹æ–‡ä»¶
    sample_dir = create_sample_files()

    try:
        # åˆå§‹åŒ– Agent
        print("\nğŸ“‹ æ­¥éª¤ 1: åˆå§‹åŒ–æ–‡ä»¶åˆ†è§£ Agent")
        agent = FileDecomposerAgent(model_name="claude-sonnet-4-5-20250929")
        print("âœ… Agent åˆå§‹åŒ–æˆåŠŸ")

        # åˆ†ææ–‡æœ¬æ–‡ä»¶
        print("\nğŸ“‹ æ­¥éª¤ 2: åˆ†ææ–‡æœ¬æ–‡ä»¶")
        txt_file = sample_dir / "ai_history.txt"

        # åŠ è½½æ–‡ä»¶
        print(f"æ­£åœ¨åŠ è½½æ–‡ä»¶: {txt_file}")
        load_result = agent._load_file_tool(str(txt_file))
        print(load_result)

        # åˆ†å‰²æ–‡æ¡£
        print("\næ­£åœ¨åˆ†å‰²æ–‡æ¡£...")
        split_result = agent._split_document_tool(str(txt_file), chunk_size=500, chunk_overlap=100)
        print(split_result)

        # åˆ†æå†…å®¹
        print("\næ­£åœ¨åˆ†æå†…å®¹...")
        analysis_result = agent._analyze_content_tool(str(txt_file))
        print(analysis_result)

        # ç”Ÿæˆæ‘˜è¦
        print("\næ­£åœ¨ç”Ÿæˆæ‘˜è¦...")
        summary_result = agent._generate_summary_tool(str(txt_file))
        print(summary_result)

        # å»ºç«‹å‘é‡ç´¢å¼•
        print("\næ­£åœ¨å»ºç«‹å‘é‡ç´¢å¼•...")
        index_result = agent.build_vector_index(str(txt_file))
        print(index_result)

        # æ¼”ç¤ºæœç´¢åŠŸèƒ½
        print("\nğŸ“‹ æ­¥éª¤ 3: æ¼”ç¤ºæœç´¢åŠŸèƒ½")
        search_result = agent._search_similar_content_tool("äººå·¥æ™ºèƒ½çš„å†å²å‘å±•", k=2)
        print(search_result)

        # æ¼”ç¤ºå¯¹è¯åŠŸèƒ½
        print("\nğŸ“‹ æ­¥éª¤ 4: æ¼”ç¤ºä¸æ–‡ä»¶å¯¹è¯")
        chat_result = agent.chat_with_file(
            str(txt_file),
            "äººå·¥æ™ºèƒ½ç»å†äº†å“ªå‡ ä¸ªä¸»è¦å‘å±•é˜¶æ®µï¼Ÿ"
        )
        print(chat_result)

        # æ¼”ç¤ºä¿¡æ¯æå–
        print("\nğŸ“‹ æ­¥éª¤ 5: æ¼”ç¤ºä¿¡æ¯æå–")
        extract_types = ["main_points", "names", "dates"]

        for extract_type in extract_types:
            print(f"\næå– {extract_type}:")
            extract_result = agent._extract_key_info_tool(str(txt_file), extract_type)
            print(extract_result)

        # åˆ†æCSVæ–‡ä»¶
        print("\nğŸ“‹ æ­¥éª¤ 6: åˆ†æCSVæ–‡ä»¶")
        csv_file = sample_dir / "tech_companies.csv"

        print(f"æ­£åœ¨åŠ è½½CSVæ–‡ä»¶: {csv_file}")
        csv_load_result = agent._load_file_tool(str(csv_file))
        print(csv_load_result)

        csv_analysis = agent._analyze_content_tool(str(csv_file))
        print("\nCSVæ–‡ä»¶åˆ†æç»“æœ:")
        print(csv_analysis)

        # åˆ†æJSONæ–‡ä»¶
        print("\nğŸ“‹ æ­¥éª¤ 7: åˆ†æJSONæ–‡ä»¶")
        json_file = sample_dir / "project_config.json"

        print(f"æ­£åœ¨åŠ è½½JSONæ–‡ä»¶: {json_file}")
        json_load_result = agent._load_file_tool(str(json_file))
        print(json_load_result)

        json_analysis = agent._analyze_content_tool(str(json_file))
        print("\nJSONæ–‡ä»¶åˆ†æç»“æœ:")
        print(json_analysis)

        # è·å–æ‰€æœ‰æ–‡ä»¶çŠ¶æ€
        print("\nğŸ“‹ æ­¥éª¤ 8: è·å–æ–‡ä»¶å¤„ç†çŠ¶æ€")
        status = agent.get_file_status()
        print("\næ‰€æœ‰æ–‡ä»¶çŠ¶æ€:")
        for file_path, file_status in status.items():
            print(f"æ–‡ä»¶: {file_path}")
            print(f"  ç±»å‹: {file_status['file_type']}")
            print(f"  å¤§å°: {file_status['file_size']:,} å­—èŠ‚")
            print(f"  å—æ•°: {file_status['chunk_count']}")
            print()

        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ ä½¿ç”¨æç¤º:")
        print("1. å°†ç¤ºä¾‹æ–‡ä»¶è·¯å¾„æ›¿æ¢ä¸ºæ‚¨è‡ªå·±çš„æ–‡ä»¶è·¯å¾„")
        print("2. æ ¹æ®éœ€è¦è°ƒæ•´åˆ†å‰²å‚æ•°ï¼ˆchunk_size, chunk_overlapï¼‰")
        print("3. ä½¿ç”¨ä¸åŒçš„ä¿¡æ¯æå–ç±»å‹æ¥è·å–ç‰¹å®šå†…å®¹")
        print("4. é€šè¿‡å‘é‡æœç´¢å¿«é€Ÿå®šä½ç›¸å…³å†…å®¹")
        print("5. ä½¿ç”¨å¯¹è¯åŠŸèƒ½è¿›è¡Œæ·±å…¥åˆ†æ")

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥:")
        print("1. æ˜¯å¦å®‰è£…äº†æ‰€æœ‰å¿…éœ€çš„ä¾èµ–")
        print("2. APIå¯†é’¥æ˜¯å¦æ­£ç¡®é…ç½®")
        print("3. æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®")

def interactive_demo():
    """äº¤äº’å¼æ¼”ç¤º"""
    print("\n" + "="*60)
    print("ğŸ® äº¤äº’å¼æ–‡ä»¶åˆ†ææ¼”ç¤º")
    print("="*60)

    try:
        agent = FileDecomposerAgent()

        print("\nè¯·è¾“å…¥è¦åˆ†æçš„æ–‡ä»¶è·¯å¾„ (æŒ‰å›è½¦ä½¿ç”¨ç¤ºä¾‹):")
        file_path = input("æ–‡ä»¶è·¯å¾„: ").strip()

        if not file_path:
            sample_dir = create_sample_files()
            file_path = str(sample_dir / "ai_history.txt")
            print(f"ä½¿ç”¨ç¤ºä¾‹æ–‡ä»¶: {file_path}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not Path(file_path).exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return

        print(f"\nğŸš€ å¼€å§‹åˆ†ææ–‡ä»¶: {file_path}")

        # åŠ è½½æ–‡ä»¶
        print("\n1. åŠ è½½æ–‡ä»¶...")
        load_result = agent._load_file_tool(file_path)
        print(load_result)

        # åˆ†æå†…å®¹
        print("\n2. åˆ†æå†…å®¹...")
        analysis_result = agent._analyze_content_tool(file_path)
        print(analysis_result)

        # å»ºç«‹ç´¢å¼•
        print("\n3. å»ºç«‹å‘é‡ç´¢å¼•...")
        index_result = agent.build_vector_index(file_path)
        print(index_result)

        # äº¤äº’å¼é—®ç­”
        print("\nğŸ’¬ ç°åœ¨æ‚¨å¯ä»¥ä¸æ–‡ä»¶å¯¹è¯ (è¾“å…¥ 'quit' é€€å‡º):")
        while True:
            question = input("\næ‚¨çš„é—®é¢˜: ").strip()
            if question.lower() in ['quit', 'exit', 'q']:
                break

            if question:
                chat_result = agent.chat_with_file(file_path, question)
                print(chat_result)

        print("\næ„Ÿè°¢ä½¿ç”¨ï¼ğŸ‘‹")

    except Exception as e:
        print(f"\nâŒ äº¤äº’å¼æ¼”ç¤ºé”™è¯¯: {e}")

if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "interactive":
        interactive_demo()
    else:
        demo_file_analysis()

        print("\n" + "="*60)
        print("ğŸ’¡ è¿è¡Œé€‰é¡¹:")
        print("1. python demo_file_decomposition.py          # è‡ªåŠ¨æ¼”ç¤º")
        print("2. python demo_file_decomposition.py interactive  # äº¤äº’å¼æ¼”ç¤º")
        print("="*60)